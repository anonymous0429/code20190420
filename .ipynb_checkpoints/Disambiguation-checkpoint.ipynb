{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Model import *\n",
    "import torch as th,torch\n",
    "import numpy as np\n",
    "from LoadData_random_sample import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['Ajay Gupta',\n",
    " 'Alok Gupta',\n",
    " 'Bin Li',\n",
    " 'Bin Yu',\n",
    " 'Bin Zhu',\n",
    " 'Bing Liu',\n",
    " 'Bo Liu',\n",
    " 'Bob Johnson',\n",
    " 'Charles Smith',\n",
    " 'Cheng Chang',\n",
    " 'Daniel Massey',\n",
    " 'David Brown',\n",
    " 'David C. Wilson',\n",
    " 'David Cooper',\n",
    " 'David E. Goldberg',\n",
    " 'David Jensen',\n",
    " 'David Levine',\n",
    " 'David Nelson',\n",
    " 'Eric Martin',\n",
    " 'Fan Wang',\n",
    " 'Fei Su',\n",
    " 'Feng Liu',\n",
    " 'Feng Pan',\n",
    " 'Frank Mueller',\n",
    " 'Gang Chen',\n",
    " 'Gang Luo',\n",
    " 'Hao Wang',\n",
    " 'Hiroshi Tanaka',\n",
    " 'Hong Xie',\n",
    " 'Hui Fang',\n",
    " 'Hui Yu',\n",
    " 'J. Yin',\n",
    " 'Jeffrey Parsons',\n",
    " 'Ji Zhang',\n",
    " 'Jianping Wang',\n",
    " 'Jie Tang',\n",
    " 'Jie Yu',\n",
    " 'Jim Gray',\n",
    " 'Jing Zhang',\n",
    " 'John Collins',\n",
    " 'John F. McDonald',\n",
    " 'John Hale',\n",
    " 'Jose M. Garcia',\n",
    " 'Juan Carlos Lopez',\n",
    " 'Kai Tang',\n",
    " 'Kai Zhang',\n",
    " 'Ke Chen',\n",
    " 'Keith Edwards',\n",
    " 'Koichi Furukawa',\n",
    " 'Kuo Zhang',\n",
    " 'Lei Chen',\n",
    " 'Lei Fang',\n",
    " 'Lei Jin',\n",
    " 'Lei Wang',\n",
    " 'Li Shen',\n",
    " 'Lu Liu',\n",
    " 'M. Rahman',\n",
    " 'Manuel Silva',\n",
    " 'Mark Davis',\n",
    " 'Michael Lang',\n",
    " 'Michael Siegel',\n",
    " 'Michael Smith',\n",
    " 'Michael Wagner',\n",
    " 'Ning Zhang',\n",
    " 'Paul Brown',\n",
    " 'Paul Wang',\n",
    " 'Peter Phillips',\n",
    " 'Philip J. Smith',\n",
    " 'Ping Zhou',\n",
    " 'Qiang shen',\n",
    " 'R. Balasubramanian',\n",
    " 'R. Cole',\n",
    " 'R. Ramesh',\n",
    " 'Rafael Alonso',\n",
    " 'Rakesh Kumar',\n",
    " 'Richard Taylor',\n",
    " 'Robert Allen',\n",
    " 'Robert Schreiber',\n",
    " 'Sanjay Jain',\n",
    " 'Satoshi Kobayashi',\n",
    " 'Shu lin',\n",
    " 'Steve King',\n",
    " 'Thomas Hermann',\n",
    " 'Thomas Meyer',\n",
    " 'Thomas Tran',\n",
    " 'Thomas Wolf',\n",
    " 'Thomas Zimmermann',\n",
    " 'Wei Xu',\n",
    " 'Wen Gao',\n",
    " 'William H. Hsu',\n",
    " 'X. Zhang',\n",
    " 'Xiaoming Wang',\n",
    " 'Xiaoyan Li',\n",
    " 'Yan Tang',\n",
    " 'Yang Wang',\n",
    " 'Yang Yu',\n",
    " 'Yi Deng',\n",
    " 'Yong Chen',\n",
    " 'Yoshio Tanaka',\n",
    " 'Young Park',\n",
    " 'Yu Zhang',\n",
    " 'Yue Zhao',\n",
    " 'Yun Wang',\n",
    " 'Z. Wang']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Accuracy=[]\n",
    "Train_Loss=[]\n",
    "Validation_Accuracy=[]\n",
    "Validation_Loss=[]\n",
    "Num_nodes=[]\n",
    "Num_edges=[]\n",
    "Num_class=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in names:\n",
    "    print(\"Disambiguating person name:\" + name)\n",
    "    edge_type,edge_list_src,edge_list_dst,num_nodes,edge_norm,vocab_size,train_idx,test_idx,inputs,labels = LoadData(name)\n",
    "    g = dgl.DGLGraph()\n",
    "    g.add_nodes(num_nodes)\n",
    "    g.add_edges(edge_list_src,edge_list_dst)\n",
    "    g.edata.update({'type': torch.LongTensor(edge_type), 'norm': torch.Tensor(edge_norm)})\n",
    "    \n",
    "    \n",
    "    \n",
    "    val_idx = test_idx\n",
    "\n",
    "    inputs = create_variable(torch.Tensor(inputs))\n",
    "    labels = create_variable(torch.LongTensor(labels))\n",
    "    \n",
    "    RNN_input_size = vocab_size\n",
    "    RNN_hidden_size = 50\n",
    "    RGCN_input_size = 25\n",
    "    RGCN_hidden_size = 20\n",
    "    Num_classes = max(labels)+1\n",
    "    Num_rels = 3\n",
    "    Num_layers = 1\n",
    "    dropout = 0.6\n",
    "    activation = F.relu\n",
    "    sequence_length = inputs.size()[1]\n",
    "    \n",
    "    Num_nodes.append(num_nodes)\n",
    "    Num_edges.append(len(edge_list_src))\n",
    "    Num_class.append(Num_classes)\n",
    "    \n",
    "    model = Model(RNN_input_size,\n",
    "                         RNN_hidden_size,\n",
    "                         RGCN_input_size,\n",
    "                         RGCN_hidden_size,\n",
    "                         Num_classes,\n",
    "                         Num_rels,\n",
    "                         Num_bases=-1,\n",
    "                         Num_hidden_layers=0,\n",
    "                         dropout=dropout)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(201):\n",
    "        optimizer.zero_grad()\n",
    "        logits = model.forward(g,inputs,sequence_length)\n",
    "        loss = criterion(logits[train_idx], labels[train_idx].long())\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            train_acc = torch.sum(logits[train_idx].argmax(dim=1) == labels[train_idx].long())\n",
    "            train_acc = train_acc.item() / len(train_idx)\n",
    "            val_loss = F.cross_entropy(logits[val_idx], labels[val_idx].long())\n",
    "            val_acc = torch.sum(logits[val_idx].argmax(dim=1) == labels[val_idx].long())\n",
    "            val_acc = val_acc.item() / len(val_idx)\n",
    "            print(\"Epoch {:05d} | \".format(epoch) +\n",
    "                  \"Train Accuracy: {:.4f} | Train Loss: {:.4f} | \".format(\n",
    "                      train_acc, loss.item()) +\n",
    "                  \"Validation Accuracy: {:.4f} | Validation loss: {:.4f}\".format(\n",
    "                      val_acc, val_loss.item()))\n",
    "    Train_Accuracy.append(train_acc)\n",
    "    Train_Loss.append(loss.item())\n",
    "    Validation_Accuracy.append(val_acc)\n",
    "    Validation_Loss.append(val_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=[]\n",
    "for i in range(len(Train_Accuracy)):\n",
    "    c.append([names[i],Train_Accuracy[i],Train_Loss[i],Validation_Accuracy[i],Validation_Loss[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('d:/result.csv', 'w', newline='') as csvfile:\n",
    "    writer  = csv.writer(csvfile)\n",
    "    for row in c:\n",
    "        writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in names:\n",
    "    print(\"Disambiguating person name:\" + name)\n",
    "    edge_type,edge_list_src,edge_list_dst,num_nodes,edge_norm,vocab_size,_,_,inputs,labels = LoadData(name)\n",
    "    g = dgl.DGLGraph()\n",
    "    g.add_nodes(num_nodes)\n",
    "    g.add_edges(edge_list_src,edge_list_dst)\n",
    "    g.edata.update({'type': torch.LongTensor(edge_type), 'norm': torch.Tensor(edge_norm)})\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    inputs = create_variable(torch.Tensor(inputs))\n",
    "    labels = create_variable(torch.LongTensor(labels))\n",
    "    \n",
    "    RNN_input_size = vocab_size\n",
    "    RNN_hidden_size = 50\n",
    "    RGCN_input_size = 25\n",
    "    RGCN_hidden_size = 20\n",
    "    Num_classes = max(labels)+1\n",
    "    Num_rels = 3\n",
    "    Num_layers = 1\n",
    "    dropout = 0.6\n",
    "    activation = F.relu\n",
    "    sequence_length = inputs.size()[1]\n",
    "    \n",
    "    Num_nodes.append(num_nodes)\n",
    "    Num_edges.append(len(edge_list_src))\n",
    "    Num_class.append(Num_classes)\n",
    "    \n",
    "    train_idx = random.sample(range(len(num_nodes)),int(num_nodes*0.8))\n",
    "    val_idx = []\n",
    "    for i in range(len(authors_split)):\n",
    "         if i not in training_idx:\n",
    "            test_idx.append(i)\n",
    "    \n",
    "    model = Model(RNN_input_size,\n",
    "                         RNN_hidden_size,\n",
    "                         RGCN_input_size,\n",
    "                         RGCN_hidden_size,\n",
    "                         Num_classes,\n",
    "                         Num_rels,\n",
    "                         Num_bases=-1,\n",
    "                         Num_hidden_layers=0,\n",
    "                         dropout=dropout)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(201):\n",
    "        optimizer.zero_grad()\n",
    "        logits = model.forward(g,inputs,sequence_length)\n",
    "        loss = criterion(logits[train_idx], labels[train_idx].long())\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            train_acc = torch.sum(logits[train_idx].argmax(dim=1) == labels[train_idx].long())\n",
    "            train_acc = train_acc.item() / len(train_idx)\n",
    "            val_loss = F.cross_entropy(logits[val_idx], labels[val_idx].long())\n",
    "            val_acc = torch.sum(logits[val_idx].argmax(dim=1) == labels[val_idx].long())\n",
    "            val_acc = val_acc.item() / len(val_idx)\n",
    "            print(\"Epoch {:05d} | \".format(epoch) +\n",
    "                  \"Train Accuracy: {:.4f} | Train Loss: {:.4f} | \".format(\n",
    "                      train_acc, loss.item()) +\n",
    "                  \"Validation Accuracy: {:.4f} | Validation loss: {:.4f}\".format(\n",
    "                      val_acc, val_loss.item()))\n",
    "    Train_Accuracy.append(train_acc)\n",
    "    Train_Loss.append(loss.item())\n",
    "    Validation_Accuracy.append(val_acc)\n",
    "    Validation_Loss.append(val_loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
