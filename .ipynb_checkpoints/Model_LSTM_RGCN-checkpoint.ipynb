{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Model import *\n",
    "import torch as th,torch\n",
    "import numpy as np\n",
    "from dgl.contrib.data import load_data\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <B>aifb dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset aifb\n",
      "Number of nodes:  8285\n",
      "Number of edges:  66371\n",
      "Number of relations:  91\n",
      "Number of classes:  4\n",
      "removing nodes that are more than 3 hops away\n"
     ]
    }
   ],
   "source": [
    "data = load_data(dataset='aifb')\n",
    "num_nodes = data.num_nodes\n",
    "num_rels = data.num_rels\n",
    "num_classes = data.num_classes\n",
    "labels = data.labels\n",
    "train_idx = data.train_idx\n",
    "# split training and validation set\n",
    "val_idx = train_idx[:len(train_idx) // 5]\n",
    "train_idx = train_idx[len(train_idx) // 5:]\n",
    "\n",
    "# edge type and normalization factor\n",
    "edge_type = torch.from_numpy(data.edge_type)\n",
    "edge_norm = torch.from_numpy(data.edge_norm).unsqueeze(1)\n",
    "\n",
    "labels = torch.from_numpy(labels).view(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create graph\n",
    "g = DGLGraph()\n",
    "g.add_nodes(num_nodes)\n",
    "g.add_edges(data.edge_src, data.edge_dst)\n",
    "g.edata.update({'type': edge_type.long(), 'norm': edge_norm})\n",
    "inputs = torch.arange(num_nodes).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aifb\n",
    "RNN_input_size = num_nodes\n",
    "RNN_hidden_size = 40\n",
    "RGCN_input_size = 30\n",
    "RGCN_hidden_size = 20\n",
    "Num_classes = 4\n",
    "Num_rels = 91\n",
    "dropout = 0.5\n",
    "activation = F.relu\n",
    "sequence_length = 1\n",
    "Num_bases=0\n",
    "lr = 0.01 # learning rate\n",
    "l2norm = 0 # L2 norm coefficient\n",
    "\n",
    "n_epochs = 25 # epochs to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(RNN_input_size,\n",
    "                     RNN_hidden_size,\n",
    "                     RGCN_input_size,\n",
    "                     RGCN_hidden_size,\n",
    "                     Num_classes,\n",
    "                     Num_rels,\n",
    "                     Num_bases=Num_bases,\n",
    "                     Num_hidden_layers=0,\n",
    "                     dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "Epoch 00000 | Train Accuracy: 0.2768 | Train Loss: 1.3819 | Validation Accuracy: 0.2857 | Validation loss: 1.3844\n",
      "Epoch 00005 | Train Accuracy: 0.8482 | Train Loss: 0.4807 | Validation Accuracy: 0.9286 | Validation loss: 0.4686\n",
      "Epoch 00010 | Train Accuracy: 0.9732 | Train Loss: 0.1165 | Validation Accuracy: 0.9643 | Validation loss: 0.2175\n",
      "Epoch 00015 | Train Accuracy: 0.9911 | Train Loss: 0.0181 | Validation Accuracy: 0.9643 | Validation loss: 0.3424\n",
      "Epoch 00020 | Train Accuracy: 1.0000 | Train Loss: 0.0004 | Validation Accuracy: 0.9643 | Validation loss: 0.5597\n",
      "Epoch 00025 | Train Accuracy: 1.0000 | Train Loss: 0.0001 | Validation Accuracy: 0.9643 | Validation loss: 0.7580\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2norm)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(\"start training...\")\n",
    "model.train()\n",
    "for epoch in range(n_epochs+1):\n",
    "    optimizer.zero_grad()\n",
    "    logits = model.forward(g,inputs,sequence_length)\n",
    "    loss = criterion(logits[train_idx], labels[train_idx].long())\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    if not epoch % 5:\n",
    "        train_acc = torch.sum(logits[train_idx].argmax(dim=1) == labels[train_idx].long())\n",
    "        train_acc = train_acc.item() / len(train_idx)\n",
    "        val_loss = F.cross_entropy(logits[val_idx], labels[val_idx].long())\n",
    "        val_acc = torch.sum(logits[val_idx].argmax(dim=1) == labels[val_idx].long())\n",
    "        val_acc = val_acc.item() / len(val_idx)\n",
    "        print(\"Epoch {:05d} | \".format(epoch) +\n",
    "              \"Train Accuracy: {:.4f} | Train Loss: {:.4f} | \".format(\n",
    "                  train_acc, loss.item()) +\n",
    "              \"Validation Accuracy: {:.4f} | Validation loss: {:.4f}\".format(\n",
    "                  val_acc, val_loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>bgs dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset bgs\n",
      "Number of nodes:  333845\n",
      "Number of edges:  2166243\n",
      "Number of relations:  207\n",
      "Number of classes:  2\n",
      "removing nodes that are more than 3 hops away\n"
     ]
    }
   ],
   "source": [
    "data = load_data(dataset='bgs')\n",
    "num_nodes = data.num_nodes\n",
    "num_rels = data.num_rels\n",
    "num_classes = data.num_classes\n",
    "labels = data.labels\n",
    "train_idx = data.train_idx\n",
    "# split training and validation set\n",
    "val_idx = train_idx[:len(train_idx) // 5]\n",
    "train_idx = train_idx[len(train_idx) // 5:]\n",
    "\n",
    "# edge type and normalization factor\n",
    "edge_type = torch.from_numpy(data.edge_type)\n",
    "edge_norm = torch.from_numpy(data.edge_norm).unsqueeze(1)\n",
    "\n",
    "labels = torch.from_numpy(labels).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create graph\n",
    "g_mutag = DGLGraph()\n",
    "g_mutag.add_nodes(num_nodes)\n",
    "g_mutag.add_edges(data.edge_src, data.edge_dst)\n",
    "g_mutag.edata.update({'type': edge_type.long(), 'norm': edge_norm})\n",
    "inputs = torch.arange(num_nodes).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mutag\n",
    "RNN_input_size = num_nodes\n",
    "RNN_hidden_size = 40\n",
    "RGCN_input_size = 30\n",
    "RGCN_hidden_size = 16\n",
    "Num_classes = num_classes\n",
    "Num_rels = num_rels\n",
    "Num_bases=40\n",
    "dropout = 0.5\n",
    "activation = F.relu\n",
    "sequence_length = 1\n",
    "n_epochs = 25 # epochs to train\n",
    "lr = 0.01 # learning rate\n",
    "l2norm = 0 # L2 norm coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mutag = Model(RNN_input_size,\n",
    "                     RNN_hidden_size,\n",
    "                     RGCN_input_size,\n",
    "                     RGCN_hidden_size,\n",
    "                     Num_classes,\n",
    "                     Num_rels,\n",
    "                     Num_bases=Num_bases,\n",
    "                     Num_hidden_layers=0,\n",
    "                     dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "Epoch 00000 | Train Accuracy: 0.4362 | Train Loss: 0.7028 | Validation Accuracy: 0.3478 | Validation loss: 0.7013\n",
      "Epoch 00001 | Train Accuracy: 0.6489 | Train Loss: 1.0428 | Validation Accuracy: 0.7391 | Validation loss: 0.7068\n",
      "Epoch 00002 | Train Accuracy: 0.6277 | Train Loss: 0.6322 | Validation Accuracy: 0.6957 | Validation loss: 0.6055\n",
      "Epoch 00003 | Train Accuracy: 0.7872 | Train Loss: 0.6315 | Validation Accuracy: 0.6957 | Validation loss: 0.6552\n",
      "Epoch 00004 | Train Accuracy: 0.6915 | Train Loss: 0.5927 | Validation Accuracy: 0.6522 | Validation loss: 0.6322\n",
      "Epoch 00005 | Train Accuracy: 0.7447 | Train Loss: 0.5288 | Validation Accuracy: 0.6957 | Validation loss: 0.5864\n",
      "Epoch 00006 | Train Accuracy: 0.8085 | Train Loss: 0.4472 | Validation Accuracy: 0.7391 | Validation loss: 0.5131\n",
      "Epoch 00007 | Train Accuracy: 0.7979 | Train Loss: 0.3722 | Validation Accuracy: 0.7826 | Validation loss: 0.4476\n",
      "Epoch 00008 | Train Accuracy: 0.8298 | Train Loss: 0.2968 | Validation Accuracy: 0.7391 | Validation loss: 0.4179\n",
      "Epoch 00009 | Train Accuracy: 0.9149 | Train Loss: 0.2127 | Validation Accuracy: 0.7826 | Validation loss: 0.3970\n",
      "Epoch 00010 | Train Accuracy: 0.9787 | Train Loss: 0.1369 | Validation Accuracy: 0.7391 | Validation loss: 0.4115\n",
      "Epoch 00011 | Train Accuracy: 0.9894 | Train Loss: 0.0788 | Validation Accuracy: 0.6957 | Validation loss: 0.4546\n",
      "Epoch 00012 | Train Accuracy: 0.9894 | Train Loss: 0.0371 | Validation Accuracy: 0.6957 | Validation loss: 0.4790\n",
      "Epoch 00013 | Train Accuracy: 1.0000 | Train Loss: 0.0134 | Validation Accuracy: 0.8696 | Validation loss: 0.5143\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-dce193f77fc1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_mutag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg_mutag\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \"\"\"\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model_mutag.parameters(), lr=lr, weight_decay=l2norm)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(\"start training...\")\n",
    "model_mutag.train()\n",
    "for epoch in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    logits = model_mutag.forward(g_mutag,inputs,sequence_length)\n",
    "    loss = criterion(logits[train_idx], labels[train_idx].long())\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    train_acc = torch.sum(logits[train_idx].argmax(dim=1) == labels[train_idx].long())\n",
    "    train_acc = train_acc.item() / len(train_idx)\n",
    "    val_loss = F.cross_entropy(logits[val_idx], labels[val_idx].long())\n",
    "    val_acc = torch.sum(logits[val_idx].argmax(dim=1) == labels[val_idx].long())\n",
    "    val_acc = val_acc.item() / len(val_idx)\n",
    "    print(\"Epoch {:05d} | \".format(epoch) +\n",
    "          \"Train Accuracy: {:.4f} | Train Loss: {:.4f} | \".format(\n",
    "              train_acc, loss.item()) +\n",
    "          \"Validation Accuracy: {:.4f} | Validation loss: {:.4f}\".format(\n",
    "              val_acc, val_loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>mutag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset mutag\n",
      "Number of nodes:  23644\n",
      "Number of edges:  172098\n",
      "Number of relations:  47\n",
      "Number of classes:  2\n",
      "removing nodes that are more than 3 hops away\n"
     ]
    }
   ],
   "source": [
    "data = load_data(dataset='mutag')\n",
    "num_nodes = data.num_nodes\n",
    "num_rels = data.num_rels\n",
    "num_classes = data.num_classes\n",
    "labels = data.labels\n",
    "train_idx = data.train_idx\n",
    "# split training and validation set\n",
    "val_idx = train_idx[:len(train_idx) // 5]\n",
    "train_idx = train_idx[len(train_idx) // 5:]\n",
    "\n",
    "# edge type and normalization factor\n",
    "edge_type = torch.from_numpy(data.edge_type)\n",
    "edge_norm = torch.from_numpy(data.edge_norm).unsqueeze(1)\n",
    "\n",
    "labels = torch.from_numpy(labels).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create graph\n",
    "g = DGLGraph()\n",
    "g.add_nodes(num_nodes)\n",
    "g.add_edges(data.edge_src, data.edge_dst)\n",
    "g.edata.update({'type': edge_type.long(), 'norm': edge_norm})\n",
    "inputs = torch.arange(num_nodes).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RNN_input_size = num_nodes\n",
    "RNN_hidden_size = 50\n",
    "RGCN_input_size = 40\n",
    "RGCN_hidden_size = 20\n",
    "Num_classes = num_classes\n",
    "Num_rels = num_rels\n",
    "dropout = 0.5\n",
    "activation = F.relu\n",
    "sequence_length = 1\n",
    "Num_bases=30\n",
    "lr = 0.01 # learning rate\n",
    "l2norm = 5e-4 # L2 norm coefficient\n",
    "n_epochs = 30 # epochs to train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(RNN_input_size,\n",
    "                     RNN_hidden_size,\n",
    "                     RGCN_input_size,\n",
    "                     RGCN_hidden_size,\n",
    "                     Num_classes,\n",
    "                     Num_rels,\n",
    "                     Num_bases=Num_bases,\n",
    "                     Num_hidden_layers=0,\n",
    "                     dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2norm)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(\"start training...\")\n",
    "model.train()\n",
    "for epoch in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    logits = model.forward(g,inputs,sequence_length)\n",
    "    loss = criterion(logits[train_idx], labels[train_idx].long())\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    train_acc = torch.sum(logits[train_idx].argmax(dim=1) == labels[train_idx].long())\n",
    "    train_acc = train_acc.item() / len(train_idx)\n",
    "    val_loss = F.cross_entropy(logits[val_idx], labels[val_idx].long())\n",
    "    val_acc = torch.sum(logits[val_idx].argmax(dim=1) == labels[val_idx].long())\n",
    "    val_acc = val_acc.item() / len(val_idx)\n",
    "    print(\"Epoch {:05d} | \".format(epoch) +\n",
    "          \"Train Accuracy: {:.4f} | Train Loss: {:.4f} | \".format(\n",
    "              train_acc, loss.item()) +\n",
    "          \"Validation Accuracy: {:.4f} | Validation loss: {:.4f}\".format(\n",
    "              val_acc, val_loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def domodel190420(dataset = 'mutag'):\n",
    "    data = load_data(dataset=dataset)\n",
    "    num_nodes = data.num_nodes\n",
    "    num_rels = data.num_rels\n",
    "    num_classes = data.num_classes\n",
    "    labels = data.labels\n",
    "    train_idx = data.train_idx\n",
    "    # split training and validation set\n",
    "    val_idx = train_idx[:len(train_idx) // 5]\n",
    "    train_idx = train_idx[len(train_idx) // 5:]\n",
    "\n",
    "    # edge type and normalization factor\n",
    "    edge_type = torch.from_numpy(data.edge_type)\n",
    "    edge_norm = torch.from_numpy(data.edge_norm).unsqueeze(1)\n",
    "\n",
    "    labels = torch.from_numpy(labels).view(-1)\n",
    "\n",
    "    # create graph\n",
    "    g = DGLGraph()\n",
    "    g.add_nodes(num_nodes)\n",
    "    g.add_edges(data.edge_src, data.edge_dst)\n",
    "    g.edata.update({'type': edge_type.long(), 'norm': edge_norm})\n",
    "    inputs = torch.arange(num_nodes).reshape(-1,1)\n",
    "\n",
    "    RNN_Hidden_Size = []\n",
    "    RGCN_Input_Size = []\n",
    "    RGCN_Hidden_Size = []\n",
    "    DROUPOUT = []\n",
    "    Num_Bases = []\n",
    "    Val_Acc = []\n",
    "    Numbase = [40]\n",
    "    if dataset == 'mutag':\n",
    "        Numbase = [0,30]\n",
    "    elif dataset == 'aifb':\n",
    "        Numbase = [0]\n",
    "    for RNN_hidden_size in [20,30,40,50]:\n",
    "        for RGCN_input_size in [10,20,30,40]:\n",
    "            for RGCN_hidden_size in [10,20,30,40]:\n",
    "                for dropout in [0,0.1,0.2,0.3,0.4,0.5]:\n",
    "                    for Num_bases in Numbase:\n",
    "                        RNN_Hidden_Size.append(RNN_hidden_size)\n",
    "                        RGCN_Input_Size.append(RGCN_input_size)\n",
    "                        RGCN_Hidden_Size.append(RGCN_hidden_size)\n",
    "                        RNN_input_size = num_nodes\n",
    "                        DROUPOUT.append(dropout)\n",
    "                        Num_Bases.append(Num_bases)\n",
    "                        # RNN_hidden_size = 50\n",
    "                        # RGCN_input_size = 40\n",
    "                        # RGCN_hidden_size = 20\n",
    "                        Num_classes = num_classes\n",
    "                        Num_rels = num_rels\n",
    "                        #dropout = 0.5\n",
    "                        activation = F.relu\n",
    "                        sequence_length = 1\n",
    "                        #Num_bases=30\n",
    "                        lr = 0.01 # learning rate\n",
    "                        l2norm = 5e-4 # L2 norm coefficient\n",
    "                        n_epochs = 50 # epochs to train\n",
    "\n",
    "                        model = Model(RNN_input_size,\n",
    "                                             RNN_hidden_size,\n",
    "                                             RGCN_input_size,\n",
    "                                             RGCN_hidden_size,\n",
    "                                             Num_classes,\n",
    "                                             Num_rels,\n",
    "                                             Num_bases=Num_bases,\n",
    "                                             Num_hidden_layers=0,\n",
    "                                             dropout=dropout)\n",
    "\n",
    "                        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2norm)\n",
    "                        criterion = nn.CrossEntropyLoss()\n",
    "                        print(\"start training...\")\n",
    "                        model.train()\n",
    "                        for epoch in range(n_epochs):\n",
    "                            optimizer.zero_grad()\n",
    "                            logits = model.forward(g,inputs,sequence_length)\n",
    "                            loss = criterion(logits[train_idx], labels[train_idx].long())\n",
    "                            loss.backward()\n",
    "\n",
    "                            optimizer.step()\n",
    "                            train_acc = torch.sum(logits[train_idx].argmax(dim=1) == labels[train_idx].long())\n",
    "                            train_acc = train_acc.item() / len(train_idx)\n",
    "                            if train_acc == 1:\n",
    "                                break\n",
    "                        val_loss = F.cross_entropy(logits[val_idx], labels[val_idx].long())\n",
    "                        val_acc = torch.sum(logits[val_idx].argmax(dim=1) == labels[val_idx].long())\n",
    "                        val_acc = val_acc.item() / len(val_idx)\n",
    "                        print(\"Epoch {:05d} | \".format(epoch) +\n",
    "                                \"Train Accuracy: {:.4f} | Train Loss: {:.4f} | \".format(\n",
    "                                    train_acc, loss.item()) +\n",
    "                                \"Validation Accuracy: {:.4f} | Validation loss: {:.4f}\".format(\n",
    "                                    val_acc, val_loss.item()))\n",
    "                        Val_Acc.append(val_acc)\n",
    "    c=[]\n",
    "    for i in range(len(RNN_Hidden_Size)):\n",
    "        c.append([RNN_Hidden_Size[i],RGCN_Input_Size[i],RGCN_Hidden_Size[i],DROUPOUT[i],Num_Bases[i],Val_Acc[i]])\n",
    "    with open('result.csv', 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for row in c:\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset aifb\n",
      "Number of nodes:  8285\n",
      "Number of edges:  66371\n",
      "Number of relations:  91\n",
      "Number of classes:  4\n",
      "removing nodes that are more than 3 hops away\n",
      "start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\lib\\site-packages\\dgl\\frame.py:204: UserWarning: Initializer is not set. Use zero initializer instead. To suppress this warning, use `set_initializer` to explicitly specify which initializer to use.\n",
      "  dgl_warning('Initializer is not set. Use zero initializer instead.'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00039 | Train Accuracy: 1.0000 | Train Loss: 0.0270 | Validation Accuracy: 0.9286 | Validation loss: 0.3041\n"
     ]
    }
   ],
   "source": [
    "datasets = ['aifb','mutag','bgs','am']\n",
    "domodel190420('aifb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
